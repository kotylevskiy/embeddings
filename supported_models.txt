# ================================
# TEXT EMBEDDING MODELS
# Transformers (PyTorch) compatible
# Tested with transformers==4.52.0 (required for Jina v3 + Qwen3)
# ================================


# -------- JINA AI --------
# Notes:
#  - compact multilingual embeddings with strong quality for their size
#  - optimized for shortâ€“medium text (queries, passages)

jinaai/jina-embeddings-v3                 # multilingual | strong-for-size
jinaai/jina-embeddings-v2-base-en         # English | baseline
jinaai/jina-embeddings-v2-base-de         # German | baseline
jinaai/jina-embeddings-v2-base-zh         # Chinese | baseline
jinaai/jina-embeddings-v2-base-code       # code | baseline-code
jinaai/jina-code-embeddings-0.5b          # code | good-code
jinaai/jina-code-embeddings-1.5b          # code | larger, slower


# -------- QWEN3 (strong default family) --------
# Notes:
#  - best general-purpose family in this list
#  - quality scales well with size

Qwen/Qwen3-Embedding-0.6B                 # multilingual | strong-default
Qwen/Qwen3-Embedding-4B                   # requires at least 16GB VRAM GPU on minimal setup or quantization
Qwen/Qwen3-Embedding-8B                   # requires at least 32GB VRAM GPU on minimal setup or quantization


# -------- E5 (classic strong baseline) --------
# Notes:
#  - competitive, well-understood behavior
#  - requires query/document prefixing for best results

intfloat/e5-small-v2                      # multilingual | lightweight-baseline
intfloat/e5-base-v2                       # multilingual | solid-baseline
intfloat/e5-large-v2                      # multilingual | strong-baseline


# -------- BGE (English-focused quality models) --------
# Notes:
#  - very strong English-only embeddings
#  - widely used in production retrieval systems

BAAI/bge-small-en-v1.5                    # English | fast-baseline
BAAI/bge-base-en-v1.5                     # English | high-quality
BAAI/bge-large-en-v1.5                    # English | very-high-quality


# -------- NOMIC (long-context oriented) --------
# Notes:
#  - tuned for long chunks / fewer chunks in RAG
#  - slightly weaker for very short queries than Qwen/BGE

nomic-ai/nomic-embed-text-v1              # multilingual | long-doc-optimized
nomic-ai/nomic-embed-text-v1.5            # multilingual | improved-long-doc


# -------- IBM GRANITE (efficient / enterprise) --------
# Notes:
#  - prioritizes efficiency and stability over peak quality

ibm-granite/granite-embedding-30m-english          # English | ultra-light
ibm-granite/granite-embedding-125m-english         # English | lightweight
ibm-granite/granite-embedding-small-english-r2     # English | improved-light
ibm-granite/granite-embedding-107m-multilingual    # multilingual | efficient
ibm-granite/granite-embedding-278m-multilingual    # multilingual | mid-quality


# -------- MULTILINGUAL / REGIONAL --------
# Notes:
#  - specialized models; use only when language/domain specificity matters

dangvantuan/vietnamese-embedding            # Vietnamese | domain-specialized
dangvantuan/vietnamese-document-embedding   # Vietnamese | document-level
dangvantuan/french-document-embedding       # French | document-level
heydariAI/persian-embeddings                # Persian | regional
Lajavaness/bilingual-embedding-small        # bilingual | regional
Lajavaness/bilingual-embedding-base         # bilingual | regional
Lajavaness/bilingual-embedding-large        # bilingual | regional


# -------- SALESFORCE SFR (precision-focused, heavy) --------
# Notes:
#  - strong retrieval quality
#  - expensive; best for high-value search or reranking

Salesforce/SFR-Embedding-2_R               # requires at least 64GB VRAM GPU on minimal setup or quantization
Salesforce/SFR-Embedding-Mistral           # requires at least 32GB VRAM GPU on minimal setup or quantization
Salesforce/SFR-Embedding-Code-400M_R       # code | strong-code
Salesforce/SFR-Embedding-Code-2B_R         # requires at least 16GB VRAM GPU on minimal setup or quantization


# -------- OPENBMB MiniCPM --------
# Notes:
#  - balanced quality, smaller footprint than Qwen3
#  - currently unsupported on CPU; see unsupported section below


# -------- NEUML / BIOMEDICAL --------
# Notes:
#  - English biomedical domain only
#  - not suitable for general-purpose text

NeuML/pubmedbert-base-embeddings                   # English biomedical | strong-domain
NeuML/pubmedbert-base-embeddings-matryoshka        # English biomedical | strong-domain | matryoshka
NeuML/bioclinical-modernbert-base-embeddings       # English clinical | strong-domain


# -------- E-COMMERCE / DOMAIN --------
# Notes:
#  - tuned for product/catalog semantics
#  - currently unsupported on CPU; see unsupported section below


# -------- CLASSIC / LIGHTWEIGHT BASELINES --------
# Notes:
#  - stable, fast, widely deployed
#  - lower ceiling than modern large models

sentence-transformers/all-MiniLM-L6-v2      # multilingual | classic-baseline
avsolatorio/NoInstruct-small-Embedding-v0   # multilingual | classic-baseline