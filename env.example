# Optional
# Hugging Face access token with read access to model weights.
# Required only for gated/private models.
HF_TOKEN=hf_put_your_token_here

# This variable is only used with Dockerfile.gpu / docker-compose.gpu.yaml
# Select a PyTorch GPU base tag from https://hub.docker.com/r/pytorch/pytorch/tags
# Use the tag only; Dockerfile.gpu resolves to "pytorch/pytorch:TAG".
# Example: 2.10.0-cuda13.0-cudnn9-runtime
PYTORCH_TAG=2.10.0-cuda13.0-cudnn9-runtime

# Device selection: auto (default), cpu, or cuda.
EMBEDDINGS_DEVICE=auto
# Optional CUDA memory cap per process (fraction of total GPU memory).
# Example: 0.5 uses up to 50% of the GPU's memory.
EMBEDDINGS_CUDA_MEMORY_FRACTION=0.5

# Max number of model bundles cached in memory.
EMBEDDINGS_MAX_LOADED_MODELS=1

# Service
# Bind host for Uvicorn inside the container/local run.
EMBEDDINGS_HOST=0.0.0.0
# Port for the API (also used for Docker port mapping).
EMBEDDINGS_PORT=11445

# Request limits
# Max texts per request batch.
EMBEDDINGS_MAX_BATCH_SIZE=16
# Max tokens per input (after tokenization).
EMBEDDINGS_MAX_INPUT_TOKENS=4096
# Whether to truncate inputs that exceed max token limit.
EMBEDDINGS_TRUNCATE_INPUTS=false
